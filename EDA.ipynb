{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a199111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EDA of Binance L2 Order Book Data (DOGEUSDT)\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03437561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (713815, 104)\n",
      "Columns: ['exchange', 'symbol', 'timestamp', 'local_timestamp', 'asks[0].price', 'asks[0].amount', 'bids[0].price', 'bids[0].amount', 'asks[1].price', 'asks[1].amount'] ...\n",
      "  exchange    symbol         timestamp   local_timestamp  asks[0].price  \\\n",
      "0  binance  DOGEUSDT  1735689601703988  1735689601703988        0.31601   \n",
      "1  binance  DOGEUSDT  1735689601974000  1735689601975707        0.31601   \n",
      "2  binance  DOGEUSDT  1735689602074000  1735689602075995        0.31601   \n",
      "3  binance  DOGEUSDT  1735689602274000  1735689602275736        0.31601   \n",
      "4  binance  DOGEUSDT  1735689602374000  1735689602375702        0.31601   \n",
      "\n",
      "   asks[0].amount  bids[0].price  bids[0].amount  asks[1].price  \\\n",
      "0          126244          0.316           87917        0.31602   \n",
      "1          126244          0.316           87917        0.31602   \n",
      "2          126244          0.316           87917        0.31602   \n",
      "3          115444          0.316           87917        0.31602   \n",
      "4          115444          0.316           87917        0.31602   \n",
      "\n",
      "   asks[1].amount  ...  bids[22].amount  asks[23].price  asks[23].amount  \\\n",
      "0           27345  ...            82614         0.31624            39742   \n",
      "1           30507  ...            82614         0.31624            39742   \n",
      "2           30507  ...            82614         0.31624            39742   \n",
      "3           32089  ...            82614         0.31624            39742   \n",
      "4           32880  ...            82614         0.31624            39742   \n",
      "\n",
      "   bids[23].price  bids[23].amount  asks[24].price  asks[24].amount  \\\n",
      "0         0.31577            32204         0.31625            44537   \n",
      "1         0.31577            32204         0.31625            44537   \n",
      "2         0.31577            32204         0.31625            44537   \n",
      "3         0.31577            32204         0.31625            44537   \n",
      "4         0.31577            32204         0.31625            44537   \n",
      "\n",
      "   bids[24].price  bids[24].amount                   datetime  \n",
      "0         0.31576           137603 2025-01-01 00:00:01.703988  \n",
      "1         0.31576           137603 2025-01-01 00:00:01.974000  \n",
      "2         0.31576           137603 2025-01-01 00:00:02.074000  \n",
      "3         0.31576           137603 2025-01-01 00:00:02.274000  \n",
      "4         0.31576           137603 2025-01-01 00:00:02.374000  \n",
      "\n",
      "[5 rows x 105 columns]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. LOAD AND INITIAL INSPECTION\n",
    "# -----------------------------\n",
    "path = \"D:/Documents/CLS/thesis/MM_sandbox/binance_book_snapshot_25_2025-01-01_DOGEUSDT.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns[:10].tolist(), \"...\")\n",
    "\n",
    "# Ensure timestamps are numeric and sorted\n",
    "df = df.sort_values(\"timestamp\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"timestamp\"], unit=\"us\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928083b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. CLEANING & VALIDATION\n",
    "# -----------------------------\n",
    "# Drop duplicate timestamps if any\n",
    "df = df.drop_duplicates(subset=[\"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# Identify and drop obviously corrupted rows\n",
    "mask_valid = df[\"asks[0].price\"].notnull() & df[\"bids[0].price\"].notnull()\n",
    "df = df.loc[mask_valid]\n",
    "\n",
    "# Check monotonicity of price ladders\n",
    "def check_monotonic(df):\n",
    "    ask_cols = [c for c in df.columns if re.search(r'asks\\[\\d+\\]\\.price', c)]\n",
    "    bid_cols = [c for c in df.columns if re.search(r'bids\\[\\d+\\]\\.price', c)]\n",
    "    ask_monotonic = (df[ask_cols].diff(axis=1).ge(0)).all(axis=1)\n",
    "    bid_monotonic = (df[bid_cols].diff(axis=1).le(0)).all(axis=1)\n",
    "    pct_invalid = 1 - ((ask_monotonic & bid_monotonic).mean())\n",
    "    print(f\"Non-monotonic snapshots: {pct_invalid*100:.2f}%\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "check_monotonic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. BASIC METRICS\n",
    "# -----------------------------\n",
    "df[\"midprice\"] = (df[\"asks[0].price\"] + df[\"bids[0].price\"]) / 2\n",
    "df[\"spread\"] = df[\"asks[0].price\"] - df[\"bids[0].price\"]\n",
    "df[\"depth_top\"] = df[\"asks[0].amount\"] + df[\"bids[0].amount\"]\n",
    "\n",
    "print(df[[\"midprice\", \"spread\", \"depth_top\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee10206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. TICK SIZE INFERENCE\n",
    "# -----------------------------\n",
    "def infer_tick_size_from_l2(df, decimals=8):\n",
    "    price_cols = [c for c in df.columns if re.search(r'(asks|bids)\\[\\d+\\]\\.price', c)]\n",
    "    all_prices = np.concatenate([df[c].values for c in price_cols])\n",
    "    diffs = np.diff(np.sort(np.unique(all_prices)))\n",
    "    diffs = diffs[diffs > 0]\n",
    "    tick = np.round(np.min(diffs), decimals)\n",
    "    diag = pd.Series(diffs).value_counts().sort_index().head(10)\n",
    "    return tick, diag\n",
    "\n",
    "tick, diag = infer_tick_size_from_l2(df)\n",
    "print(f\"Inferred tick size: {tick}\")\n",
    "print(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. EMPIRICAL DISTRIBUTIONS\n",
    "# -----------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df[\"spread\"], bins=100, kde=True)\n",
    "plt.title(\"Distribution of Top-of-Book Spread\")\n",
    "plt.xlabel(\"Spread\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df[\"midprice\"].pct_change().dropna(), bins=200, kde=True)\n",
    "plt.title(\"Empirical Density of Midprice Returns\")\n",
    "plt.xlabel(\"Return\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(x=\"datetime\", y=\"midprice\", data=df.sample(n=5000))\n",
    "plt.title(\"Midprice Evolution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817811cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. DEPTH PROFILE (SNAPSHOT)\n",
    "# -----------------------------\n",
    "snap = df.iloc[len(df)//2]  # middle of day snapshot\n",
    "depth = pd.DataFrame({\n",
    "    \"ask_price\": [snap[f\"asks[{i}].price\"] for i in range(24)],\n",
    "    \"ask_amount\": [snap[f\"asks[{i}].amount\"] for i in range(24)],\n",
    "    \"bid_price\": [snap[f\"bids[{i}].price\"] for i in range(24)],\n",
    "    \"bid_amount\": [snap[f\"bids[{i}].amount\"] for i in range(24)],\n",
    "})\n",
    "depth = depth.sort_values(\"ask_price\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(depth[\"bid_price\"], depth[\"bid_amount\"].cumsum(), label=\"Bids\")\n",
    "plt.plot(depth[\"ask_price\"], depth[\"ask_amount\"].cumsum(), label=\"Asks\")\n",
    "plt.title(\"Order Book Depth Profile (midday snapshot)\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Cumulative Size\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23844064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. TEMPORAL VOLATILITY ANALYSIS\n",
    "# -----------------------------\n",
    "df[\"return\"] = df[\"midprice\"].pct_change()\n",
    "window = 60  # 60-second rolling window\n",
    "df[\"volatility\"] = df[\"return\"].rolling(window).std() * np.sqrt(60)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df[\"datetime\"], df[\"volatility\"], linewidth=0.8)\n",
    "plt.title(\"Rolling Realized Volatility (per minute)\")\n",
    "plt.ylabel(\"σ\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. SAVE CLEANED DATA\n",
    "# -----------------------------\n",
    "clean_path = \"dogeusdt_l2_clean.csv\"\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved cleaned data to {clean_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df already cleaned and has columns:\n",
    "# ['timestamp', 'datetime', 'asks[0].price', 'asks[0].amount', 'bids[0].price', 'bids[0].amount', ...]\n",
    "\n",
    "# -----------------------------\n",
    "# 9. MICROSTRUCTURE METRICS\n",
    "# -----------------------------\n",
    "\n",
    "def compute_orderbook_imbalance(df, levels=5):\n",
    "    \"\"\"Compute top-N order book imbalance.\"\"\"\n",
    "    ask_vol = df[[f\"asks[{i}].amount\" for i in range(levels)]].sum(axis=1)\n",
    "    bid_vol = df[[f\"bids[{i}].amount\" for i in range(levels)]].sum(axis=1)\n",
    "    return (bid_vol - ask_vol) / (bid_vol + ask_vol)\n",
    "\n",
    "def compute_weighted_midprice(df, levels=5):\n",
    "    \"\"\"Weighted midprice over top-N levels.\"\"\"\n",
    "    ask_price = df[[f\"asks[{i}].price\" for i in range(levels)]]\n",
    "    bid_price = df[[f\"bids[{i}].price\" for i in range(levels)]]\n",
    "    ask_vol = df[[f\"asks[{i}].amount\" for i in range(levels)]]\n",
    "    bid_vol = df[[f\"bids[{i}].amount\" for i in range(levels)]]\n",
    "    ask_wavg = (ask_price * ask_vol).sum(axis=1) / ask_vol.sum(axis=1)\n",
    "    bid_wavg = (bid_price * bid_vol).sum(axis=1) / bid_vol.sum(axis=1)\n",
    "    return (ask_wavg + bid_wavg) / 2\n",
    "\n",
    "def compute_quote_slope(df, levels=5):\n",
    "    \"\"\"Quote slope: price-volume slope over top-N levels.\"\"\"\n",
    "    slopes = []\n",
    "    for i in range(levels - 1):\n",
    "        pa = df[f\"asks[{i+1}].price\"] - df[f\"asks[{i}].price\"]\n",
    "        va = df[f\"asks[{i+1}].amount\"] - df[f\"asks[{i}].amount\"]\n",
    "        pb = df[f\"bids[{i}].price\"] - df[f\"bids[{i+1}].price\"]\n",
    "        vb = df[f\"bids[{i}].amount\"] - df[f\"bids[{i+1}].amount\"]\n",
    "        slope = (va / pa.abs() + vb / pb.abs()) / 2\n",
    "        slopes.append(slope)\n",
    "    return np.nanmean(slopes, axis=0)\n",
    "\n",
    "# Compute metrics\n",
    "df[\"imbalance\"] = compute_orderbook_imbalance(df, levels=5)\n",
    "df[\"wmp\"] = compute_weighted_midprice(df, levels=5)\n",
    "df[\"quote_slope\"] = compute_quote_slope(df, levels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d22eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10. MICROSTRUCTURE INSIGHTS\n",
    "# -----------------------------\n",
    "\n",
    "# A) Imbalance distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df[\"imbalance\"], bins=100, kde=True)\n",
    "plt.title(\"Distribution of Top-5 Order Book Imbalance\")\n",
    "plt.xlabel(\"Imbalance (Bid - Ask) / (Bid + Ask)\")\n",
    "plt.show()\n",
    "\n",
    "# B) Weighted midprice vs top-of-book midprice\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(x=df[\"midprice\"], y=df[\"wmp\"], s=3, alpha=0.3)\n",
    "plt.title(\"Weighted Midprice vs Top-of-Book Midprice\")\n",
    "plt.xlabel(\"Top-of-Book Midprice\")\n",
    "plt.ylabel(\"Weighted Midprice\")\n",
    "plt.show()\n",
    "\n",
    "# C) Quote slope distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df[\"quote_slope\"].dropna(), bins=100, kde=True)\n",
    "plt.title(\"Quote Slope Distribution (Top 5 Levels)\")\n",
    "plt.xlabel(\"Slope\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 11. MIDPRICE IMPACT ANALYSIS\n",
    "# -----------------------------\n",
    "# How does imbalance predict future midprice change?\n",
    "\n",
    "horizon = 10  # ticks ahead\n",
    "df[\"future_mid\"] = df[\"midprice\"].shift(-horizon)\n",
    "df[\"mid_change\"] = df[\"future_mid\"] / df[\"midprice\"] - 1\n",
    "\n",
    "valid = df.dropna(subset=[\"imbalance\", \"mid_change\"])\n",
    "corr, _ = pearsonr(valid[\"imbalance\"], valid[\"mid_change\"])\n",
    "print(f\"Correlation between imbalance and future midprice change (h={horizon}): {corr:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.regplot(x=\"imbalance\", y=\"mid_change\", data=valid.sample(20000))\n",
    "plt.title(f\"Predictive Relationship: Imbalance vs Future Midprice Change (h={horizon})\")\n",
    "plt.xlabel(\"Imbalance (Top-5)\")\n",
    "plt.ylabel(\"Future Midprice Change\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 12. AUTOCORRELATION STRUCTURE\n",
    "# -----------------------------\n",
    "returns = df[\"midprice\"].pct_change().dropna()\n",
    "spreads = df[\"spread\"].dropna()\n",
    "\n",
    "acf_returns = acf(returns, nlags=50, fft=True)\n",
    "acf_spreads = acf(spreads, nlags=50, fft=True)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(acf_returns)\n",
    "plt.title(\"Autocorrelation of Midprice Returns\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(acf_spreads)\n",
    "plt.title(\"Autocorrelation of Spreads\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 13. LIQUIDITY HEATMAPS\n",
    "# -----------------------------\n",
    "# Aggregate average liquidity at each level across the day\n",
    "ask_amounts = [f\"asks[{i}].amount\" for i in range(24)]\n",
    "bid_amounts = [f\"bids[{i}].amount\" for i in range(24)]\n",
    "avg_ask = df[ask_amounts].mean().values\n",
    "avg_bid = df[bid_amounts].mean().values[::-1]  # reverse for plotting symmetry\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.arange(-24, 0), avg_bid, color=\"blue\", alpha=0.5, label=\"Bids\")\n",
    "plt.bar(np.arange(0, 24), avg_ask, color=\"red\", alpha=0.5, label=\"Asks\")\n",
    "plt.title(\"Average Depth per Level\")\n",
    "plt.xlabel(\"Book Level (negative = bids, positive = asks)\")\n",
    "plt.ylabel(\"Mean Size\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of average price levels\n",
    "ask_prices = df[[f\"asks[{i}].price\" for i in range(24)]].mean()\n",
    "bid_prices = df[[f\"bids[{i}].price\" for i in range(24)]].mean()\n",
    "\n",
    "heat_df = pd.DataFrame({\n",
    "    \"bid_price\": bid_prices.values[::-1],\n",
    "    \"ask_price\": ask_prices.values,\n",
    "    \"bid_size\": avg_bid,\n",
    "    \"ask_size\": avg_ask\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(\n",
    "    np.log1p(np.vstack([avg_bid[::-1], avg_ask])),\n",
    "    cmap=\"coolwarm\", xticklabels=False, yticklabels=[\"Bids\", \"Asks\"]\n",
    ")\n",
    "plt.title(\"Log-Liquidity Heatmap (Average Depth Profile)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def00b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 14. SAVE EXTENDED METRICS\n",
    "# -----------------------------\n",
    "df.to_csv(\"dogeusdt_l2_clean_enriched.csv\", index=False)\n",
    "print(\"✅ Saved enriched dataset with microstructure metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9217ed1",
   "metadata": {},
   "source": [
    "Empirical Density & Statistical Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Empirical Densities and Statistical Plots (Zhang Mian-style)\n",
    "# ============================================\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Ensure we have returns, spreads, depth columns\n",
    "df[\"return\"] = df[\"midprice\"].pct_change()\n",
    "df[\"log_return\"] = np.log(df[\"midprice\"]).diff()\n",
    "df[\"depth_top\"] = df[\"asks[0].amount\"] + df[\"bids[0].amount\"]\n",
    "\n",
    "# --------------------------------------------\n",
    "# 1. MIDPRICE EVOLUTION\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df[\"datetime\"], df[\"midprice\"], linewidth=0.8)\n",
    "plt.title(\"Midprice Evolution (DOGEUSDT)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Midprice\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2. EMPIRICAL DENSITY OF LOG RETURNS\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"log_return\"].dropna(), bins=300, kde=True, stat=\"density\")\n",
    "plt.title(\"Empirical Density of Log Returns\")\n",
    "plt.xlabel(\"Log Return\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# Overlay Normal PDF for comparison\n",
    "mu, sigma = df[\"log_return\"].mean(), df[\"log_return\"].std()\n",
    "x = np.linspace(df[\"log_return\"].min(), df[\"log_return\"].max(), 200)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma), \"r--\", label=\"Normal PDF\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3. QQ-PLOT OF LOG RETURNS\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(6,6))\n",
    "stats.probplot(df[\"log_return\"].dropna(), dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ-Plot of Log Returns (Normal Reference)\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4. SPREAD DISTRIBUTION\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"spread\"], bins=200, kde=True)\n",
    "plt.title(\"Distribution of Bid-Ask Spread\")\n",
    "plt.xlabel(\"Spread\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5. DEPTH DISTRIBUTION\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(np.log1p(df[\"depth_top\"]), bins=200, kde=True)\n",
    "plt.title(\"Distribution of Log(Depth) at Top Level\")\n",
    "plt.xlabel(\"log(asks[0].amount + bids[0].amount)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6. AUTOCORRELATION OF RETURNS\n",
    "# --------------------------------------------\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "returns = df[\"log_return\"].dropna()\n",
    "acf_vals = acf(returns, nlags=50, fft=True)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(acf_vals)\n",
    "plt.title(\"Autocorrelation of Log Returns\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 7. VOLATILITY CLUSTERING (Rolling Std)\n",
    "# --------------------------------------------\n",
    "window = 300  # rolling window size (e.g. 5 minutes if 1Hz data)\n",
    "df[\"rolling_vol\"] = df[\"log_return\"].rolling(window).std()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df[\"datetime\"], df[\"rolling_vol\"], linewidth=0.8)\n",
    "plt.title(f\"Rolling Volatility (window={window})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"σ\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 8. JOINT DISTRIBUTION: SPREAD VS DEPTH\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.kdeplot(\n",
    "    x=np.log1p(df[\"spread\"]),\n",
    "    y=np.log1p(df[\"depth_top\"]),\n",
    "    fill=True, cmap=\"mako\"\n",
    ")\n",
    "plt.title(\"Joint Density: log(Spread) vs log(Depth)\")\n",
    "plt.xlabel(\"log(Spread)\")\n",
    "plt.ylabel(\"log(Depth)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41608a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# --- 1) durations in seconds (timestamps are in microseconds)\n",
    "time_in_s = (df[\"timestamp\"].astype(float) / 1e6).to_numpy()\n",
    "durations = np.diff(time_in_s)\n",
    "durations = durations[durations > 0]\n",
    "\n",
    "# --- 2) (optional) inspect scale of your data\n",
    "print(\n",
    "    \"Duration stats (s):\",\n",
    "    f\"min={durations.min():.6g}\",\n",
    "    f\"median={np.median(durations):.6g}\",\n",
    "    f\"mean={durations.mean():.6g}\",\n",
    "    f\"max={durations.max():.6g}\",\n",
    ")\n",
    "\n",
    "# --- 3) fit exponential: MLE for loc and scale\n",
    "# If you want to force the fit to start at 0 exactly, use: stats.expon.fit(durations, floc=0)\n",
    "loc, scale = stats.expon.fit(durations, floc=0)  # loc≈0, scale = 1/λ\n",
    "lam = 1.0 / scale\n",
    "print(f\"Exponential fit: loc={loc:.6g}, scale={scale:.6g}  ->  λ={lam:.6g} per second\")\n",
    "\n",
    "# --- 4) choose an x-range that shows the bulk of mass\n",
    "p99 = np.percentile(durations, 99)          # trim extreme outliers\n",
    "xmax = max(p99, 1e-3) * 5                   # a bit wider for visibility\n",
    "x = np.linspace(0, xmax, 600)\n",
    "y = stats.expon.pdf(x, loc=loc, scale=scale)\n",
    "\n",
    "# --- 5) plot histogram + fitted exponential pdf (Zhang-Mian style)\n",
    "fig, ax = plt.subplots(figsize=(6, 4.5))\n",
    "ax.hist(durations, bins=100, density=True, label=\"empirical distribution\", alpha=0.8)\n",
    "ax.plot(x, y, label=\"fitted distribution\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"duration\")\n",
    "ax.set_ylabel(\"distribution\")\n",
    "ax.set_title(\"The exponential fit of the histogram of the Duration\")\n",
    "\n",
    "# Use the classic fixed axis from the screenshot OR the auto one above.\n",
    "# (uncomment ONE of the following):\n",
    "# ax.set_xlim((0, 30)); ax.set_ylim((0, 0.2))     # original screenshot styling\n",
    "ax.set_xlim((0, xmax)); ax.set_ylim(bottom=0)     # auto scaling for sub-second data\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Convert timestamps (µs → s)\n",
    "time_in_s = (df[\"timestamp\"].astype(float) / 1e6).to_numpy()\n",
    "\n",
    "# --- 2) Compute positive durations\n",
    "durations = np.diff(time_in_s)\n",
    "durations = durations[durations > 0]\n",
    "\n",
    "print(\"Duration stats (s):\")\n",
    "print(f\"min={durations.min():.6f}, max={durations.max():.6f}, median={np.median(durations):.6f}\")\n",
    "\n",
    "# --- 3) Choose reasonable x-limits automatically\n",
    "p99 = np.percentile(durations, 99)  # cut extreme outliers\n",
    "xmax = max(p99, 1e-3) * 10  # a bit wider for visibility\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# (a) Linear scale\n",
    "axes[0].hist(durations, bins=200, density=True)\n",
    "axes[0].set_xlim((0, xmax))\n",
    "axes[0].set_xlabel(\"duration (s)\")\n",
    "axes[0].set_ylabel(\"density\")\n",
    "axes[0].set_title(f\"Histogram of durations (0–{xmax:.4f}s)\")\n",
    "\n",
    "# (b) Log-log scale\n",
    "axes[1].hist(durations, bins=200, density=True, log=True)\n",
    "axes[1].set_xlabel(\"duration (s)\")\n",
    "axes[1].set_ylabel(\"log-density\")\n",
    "axes[1].set_title(\"Histogram of durations (log-log)\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].grid(True, which=\"both\", ls=\"--\", lw=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06505a40",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EDA of Binance L2 Order Book Data (DOGEUSDT)\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. LOAD AND INITIAL INSPECTION\n",
    "# -----------------------------\n",
    "path = \"D:/Documents/CLS/thesis/MM_sandbox/binance_book_snapshot_25_2025-01-01_DOGEUSDT.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns[:10].tolist(), \"...\")\n",
    "print(df.head())\n",
    "\n",
    "# Ensure timestamps are numeric and sorted\n",
    "df = df.sort_values(\"timestamp\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"timestamp\"], unit=\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d626a2",
   "metadata": {},
   "source": [
    "### Poisson arrival parameters estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from estimate_exponential_arrivals import load_minimal_df, estimate_once, auto_horizon_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load data ---\n",
    "df = pd.read_csv(\"D:/Documents/CLS/thesis/MM_sandbox/binance_book_snapshot_25_2025-01-01_DOGEUSDT.csv\")\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664071af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent column names\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Convert timestamps if in microseconds\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "    \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77397979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load your data (CSV with columns like asks[0].price, bids[0].price + a timestamp)\n",
    "core = load_minimal_df(Path(\"D:/Documents/CLS/thesis/MM_sandbox/binance_book_snapshot_25_2025-01-01_DOGEUSDT.csv\")).sort_values(\"__ts\").reset_index(drop=True)\n",
    "\n",
    "# 2) Choose settings\n",
    "deltas = [0,1,2,3,4,5]    # in ticks\n",
    "T = auto_horizon_T(core[\"__ts\"])  # right-censoring horizon (sec)\n",
    "tick_override = None      # or e.g., 0.01 if you know the tick\n",
    "outdir = Path(\"out_demo\"); outdir.mkdir(exist_ok=True)\n",
    "\n",
    "# 3) Estimate (once, globally)\n",
    "rates, fit = estimate_once(core, deltas, T, tick_override, make_plots=True, outdir=outdir, label_suffix=\"global\", show_plots=True)\n",
    "\n",
    "# 4) Inspect\n",
    "display(rates.head(10))\n",
    "display(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Midprice = (best_bid + best_ask) / 2\n",
    "df['best_bid'] = df['bids[0].price']\n",
    "df['best_ask'] = df['asks[0].price']\n",
    "df['midprice'] = (df['best_bid'] + df['best_ask']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e311e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tick size known, set it here (you can estimate it later)\n",
    "tick_size = 0.0001  # example\n",
    "\n",
    "# Compute δ for each level\n",
    "for i in range(5):  # use top 5 levels as example\n",
    "    df[f'bid_delta_{i}'] = (df['midprice'] - df[f'bids[{i}].price']) / tick_size\n",
    "    df[f'ask_delta_{i}'] = (df[f'asks[{i}].price'] - df['midprice']) / tick_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb88972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate arrival events by price movements\n",
    "df['bid_price_change'] = df['best_bid'].diff().abs() > 0\n",
    "df['ask_price_change'] = df['best_ask'].diff().abs() > 0\n",
    "\n",
    "# Estimate fill rates as frequency of changes per time unit\n",
    "df['time_diff'] = df['timestamp'].diff().dt.total_seconds()\n",
    "df['bid_arrival_rate'] = df['bid_price_change'] / df['time_diff']\n",
    "df['ask_arrival_rate'] = df['ask_price_change'] / df['time_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) infer tick (robust-ish)\n",
    "def infer_tick_from_prices(prices: pd.Series) -> float:\n",
    "    p = np.sort(prices.dropna().unique())\n",
    "    diffs = np.diff(p)\n",
    "    diffs = diffs[diffs > 0]\n",
    "    return float(np.round(np.quantile(diffs, 0.05), 8)) if len(diffs) else np.nan\n",
    "\n",
    "tick_bid = infer_tick_from_prices(df['bids[0].price'])\n",
    "tick_ask = infer_tick_from_prices(df['asks[0].price'])\n",
    "tick = np.nanmin([tick_bid, tick_ask])\n",
    "\n",
    "# 2) convert to integer ticks (≥0)\n",
    "df['delta_ask_ticks'] = np.maximum(0, np.rint((df['asks[0].price'] - df['midprice']) / tick)).astype(int)\n",
    "df['delta_bid_ticks'] = np.maximum(0, np.rint((df['midprice'] - df['bids[0].price']) / tick)).astype(int)\n",
    "\n",
    "# 3) approximate arrival rates (price-change proxy)\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "df['dt'] = df['timestamp'].diff().dt.total_seconds().replace(0, np.nan)\n",
    "df['bid_arrival_rate'] = (df['bids[0].price'].diff().abs() > 0).astype(int) / df['dt']\n",
    "df['ask_arrival_rate'] = (df['asks[0].price'].diff().abs() > 0).astype(int) / df['dt']\n",
    "\n",
    "ask_group = (df[['delta_ask_ticks','ask_arrival_rate']]\n",
    "             .dropna()\n",
    "             .groupby('delta_ask_ticks', as_index=False)\n",
    "             .agg(lambda_mean=('ask_arrival_rate','mean')))\n",
    "\n",
    "bid_group = (df[['delta_bid_ticks','bid_arrival_rate']]\n",
    "             .dropna()\n",
    "             .groupby('delta_bid_ticks', as_index=False)\n",
    "             .agg(lambda_mean=('bid_arrival_rate','mean')))\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "if not ask_group.empty:\n",
    "    plt.plot(ask_group['delta_ask_ticks'], ask_group['lambda_mean'], marker='o', label='Ask side')\n",
    "if not bid_group.empty:\n",
    "    plt.plot(bid_group['delta_bid_ticks'], bid_group['lambda_mean'], marker='o', label='Bid side')\n",
    "plt.xlabel('Distance δ (ticks)')\n",
    "plt.ylabel('Arrival rate λ (1/s)')\n",
    "plt.title('Empirical λ(δ)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exp_model(delta, A, k):\n",
    "    return A * np.exp(-k * delta)\n",
    "\n",
    "# --- build integer tick distances (assumes df has best_bid/best_ask/mid and tick) ---\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# arrival proxies (avoid dt=0)\n",
    "df['dt'] = df['timestamp'].diff().dt.total_seconds().replace(0, np.nan)\n",
    "df['ask_arrival_rate'] = (df['asks[0].price'].diff().abs() > 0).astype(int) / df['dt']\n",
    "df['bid_arrival_rate'] = (df['bids[0].price'].diff().abs() > 0).astype(int) / df['dt']\n",
    "\n",
    "# integer tick deltas\n",
    "df['delta_ask_ticks'] = np.maximum(0, np.rint((df['asks[0].price'] - df['midprice']) / tick)).astype(int)\n",
    "df['delta_bid_ticks'] = np.maximum(0, np.rint((df['midprice'] - df['bids[0].price']) / tick)).astype(int)\n",
    "\n",
    "# group by delta; use counts as weights\n",
    "ask_g = (df[['delta_ask_ticks','ask_arrival_rate']]\n",
    "         .dropna()\n",
    "         .groupby('delta_ask_ticks', as_index=False)\n",
    "         .agg(lambda_mean=('ask_arrival_rate','mean'),\n",
    "              n=('ask_arrival_rate','size')))\n",
    "\n",
    "bid_g = (df[['delta_bid_ticks','bid_arrival_rate']]\n",
    "         .dropna()\n",
    "         .groupby('delta_bid_ticks', as_index=False)\n",
    "         .agg(lambda_mean=('bid_arrival_rate','mean'),\n",
    "              n=('bid_arrival_rate','size')))\n",
    "\n",
    "# choose one side to fit (e.g., ask)\n",
    "g = ask_g.rename(columns={'delta_ask_ticks':'delta','lambda_mean':'lambda'})\n",
    "g = g[(g['lambda']>0) & np.isfinite(g['lambda'])]\n",
    "\n",
    "if len(g) < 3:\n",
    "    raise ValueError(\"Not enough (δ, λ) points to fit. Increase bins/levels/time window.\")\n",
    "\n",
    "x = g['delta'].astype(float).values\n",
    "y = g['lambda'].astype(float).values\n",
    "w = g['n'].astype(float).values  # optional weights (more obs -> higher weight)\n",
    "\n",
    "# good initial guesses and bounds\n",
    "A0 = y.max()\n",
    "k0 = 0.1\n",
    "popt, pcov = curve_fit(exp_model, x, y, p0=(A0, k0), bounds=([0, 0], [np.inf, np.inf]))\n",
    "A_hat, k_hat = popt\n",
    "print(f\"Estimated A = {A_hat:.6g}, k = {k_hat:.6g}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mysimenv)",
   "language": "python",
   "name": "mysimenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
